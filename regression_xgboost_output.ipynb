{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1) Data preprocessing and feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load JSONL file\n",
        "def load_jsonl(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = [json.loads(line) for line in f]\n",
        "    print(f\"Loaded {len(data)} records from {file_path}.\")\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract features from the training dataset\n",
        "def extract_training_data(training_data):\n",
        "    post_data = []\n",
        "    profile_data = []\n",
        "\n",
        "    for item in training_data:\n",
        "        # Extract profile-level data\n",
        "        profile = item.get('profile', {})\n",
        "        profile_id = profile.get('id', 'UNKNOWN')\n",
        "        username = profile.get('username', 'UNKNOWN')\n",
        "        post_count = profile.get('post_count', 0)\n",
        "        follower_count = profile.get('follower_count', 0)\n",
        "        highlight_reel_count = profile.get('highlight_reel_count', 0)\n",
        "        \n",
        "        # Save profile-level data\n",
        "        profile_data.append({\n",
        "            'profile_id': profile_id,\n",
        "            'username': username,\n",
        "            'post_count': post_count,\n",
        "            'follower_count': follower_count,\n",
        "            'highlight_reel_count': highlight_reel_count\n",
        "        })\n",
        "        \n",
        "        # Extract post-level data\n",
        "        for post in item.get('posts', []):\n",
        "            post_id = post.get('id', 'UNKNOWN')\n",
        "            caption = post.get('caption', '')\n",
        "            like_count = post.get('like_count')\n",
        "            comments_count = post.get('comments_count', 0)\n",
        "            media_type = post.get('media_type', 'UNKNOWN')\n",
        "            \n",
        "            # Include only if like_count is not missing\n",
        "            if like_count is not None:\n",
        "                post_data.append({\n",
        "                    'post_id': post_id,\n",
        "                    'caption': caption,\n",
        "                    'like_count': like_count,\n",
        "                    'comments_count': comments_count,\n",
        "                    'media_type': media_type,\n",
        "                    'profile_id': profile_id  # Link to profile\n",
        "                })\n",
        "    \n",
        "    # Convert to DataFrames\n",
        "    df_posts = pd.DataFrame(post_data)\n",
        "    df_profiles = pd.DataFrame(profile_data).drop_duplicates(subset='profile_id')  # Avoid duplicate profiles\n",
        "    \n",
        "    # Join profile data to posts based on profile_id\n",
        "    df_combined = df_posts.merge(df_profiles, on='profile_id', how='left')\n",
        "    \n",
        "    print(f\"Extracted {len(df_combined)} rows of data after combining posts and profiles.\")\n",
        "    return df_combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "def preprocess_combined_data(df):\n",
        "    # Ensure no None or NaN values in the 'caption' column\n",
        "    df['caption'] = df['caption'].fillna('').astype(str)\n",
        "\n",
        "    # Tokenize captions for Word2Vec\n",
        "    tokenized_captions = df['caption'].apply(lambda x: x.split())\n",
        "\n",
        "    # Train a Word2Vec model\n",
        "    word2vec_model = Word2Vec(sentences=tokenized_captions, vector_size=100, window=7, min_count=1, sg=0)\n",
        "\n",
        "    # Function to average word vectors for each caption\n",
        "    def get_caption_vector(tokens):\n",
        "        vectors = [word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv]\n",
        "        if vectors:\n",
        "            return np.mean(vectors, axis=0)\n",
        "        else:\n",
        "            return np.zeros(word2vec_model.vector_size)\n",
        "\n",
        "    # Apply the function to compute caption embeddings\n",
        "    caption_vectors = np.array([get_caption_vector(tokens) for tokens in tokenized_captions])\n",
        "    print(f\"Word2Vec caption vectors shape: {caption_vectors.shape}\")\n",
        "\n",
        "    # Encode media type as a numerical feature\n",
        "    media_type_mapping = {media: idx for idx, media in enumerate(df['media_type'].unique())}\n",
        "    df['media_type_encoded'] = df['media_type'].map(media_type_mapping)\n",
        "\n",
        "    # Handle heavy-tailed distributions with log transformation\n",
        "    df['comments_count_log'] = np.log1p(df['comments_count'])\n",
        "\n",
        "    # Feature engineering: followers per post and comments-to-followers ratio\n",
        "    df['followers_per_post'] = df['follower_count'] / (df['post_count'] + 1)  # Avoid division by zero\n",
        "    df['comments_to_followers_ratio'] = df['comments_count'] / (df['follower_count'] + 1)\n",
        "\n",
        "    # Replace infinite values and handle NaNs\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    # Remove 'like_count' from training and set target variable\n",
        "    y = df['like_count']\n",
        "    df.drop('like_count', axis=1, inplace=True)\n",
        "\n",
        "    # Combine features: Word2Vec vectors and other engineered features\n",
        "    features = pd.DataFrame(caption_vectors)\n",
        "    features['comments_count_log'] = df['comments_count_log']\n",
        "    features['media_type_encoded'] = df['media_type_encoded']\n",
        "    features['post_count'] = df['post_count']\n",
        "    features['follower_count'] = df['follower_count']\n",
        "    features['highlight_reel_count'] = df['highlight_reel_count']\n",
        "    features['followers_per_post'] = df['followers_per_post']\n",
        "    features['comments_to_followers_ratio'] = df['comments_to_followers_ratio']\n",
        "\n",
        "    # Ensure all column names are strings\n",
        "    features.columns = features.columns.astype(str)\n",
        "    print(\"Feature columns:\", features.columns)\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "    print(\"Features standardized.\")\n",
        "\n",
        "    # Return processed features and target variable\n",
        "    return features_scaled, np.log1p(y), word2vec_model, scaler, features.columns, media_type_mapping\n",
        "\n",
        "# Example call (assuming `df` is your DataFrame)\n",
        "# features_scaled, y_log, word2vec_model, scaler = preprocess_combined_data(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 5415 records from training-dataset.jsonl.\n",
            "Extracted 183083 rows of data after combining posts and profiles.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>caption</th>\n",
              "      <th>like_count</th>\n",
              "      <th>comments_count</th>\n",
              "      <th>media_type</th>\n",
              "      <th>profile_id</th>\n",
              "      <th>username</th>\n",
              "      <th>post_count</th>\n",
              "      <th>follower_count</th>\n",
              "      <th>highlight_reel_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17990918969458720</td>\n",
              "      <td>Cumhuriyetimizin 100.yƒ±lƒ± kutlu olsun‚ôæÔ∏èüáπüá∑</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0</td>\n",
              "      <td>IMAGE</td>\n",
              "      <td>3170700063</td>\n",
              "      <td>deparmedya</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1167</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18219250732221045</td>\n",
              "      <td>Oriflame Duologi Lansmanƒ± #isve√ßtengeleng√ºzell...</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>3170700063</td>\n",
              "      <td>deparmedya</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1167</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18311380465102328</td>\n",
              "      <td>#oriflameilesa√ßbakƒ±mdevrimi ‚úåÔ∏è</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>3170700063</td>\n",
              "      <td>deparmedya</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1167</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18089518138361507</td>\n",
              "      <td>‚úåÔ∏è#oriflameilesa√ßbakƒ±mdevrimi 07Agustos‚Äô23 ori...</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>3170700063</td>\n",
              "      <td>deparmedya</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1167</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18012743929758497</td>\n",
              "      <td>07 Agustos‚Äô23 #oriflameturkiye #duoloji</td>\n",
              "      <td>21.0</td>\n",
              "      <td>0</td>\n",
              "      <td>VIDEO</td>\n",
              "      <td>3170700063</td>\n",
              "      <td>deparmedya</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1167</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             post_id                                            caption  \\\n",
              "0  17990918969458720          Cumhuriyetimizin 100.yƒ±lƒ± kutlu olsun‚ôæÔ∏èüáπüá∑   \n",
              "1  18219250732221045  Oriflame Duologi Lansmanƒ± #isve√ßtengeleng√ºzell...   \n",
              "2  18311380465102328                     #oriflameilesa√ßbakƒ±mdevrimi ‚úåÔ∏è   \n",
              "3  18089518138361507  ‚úåÔ∏è#oriflameilesa√ßbakƒ±mdevrimi 07Agustos‚Äô23 ori...   \n",
              "4  18012743929758497            07 Agustos‚Äô23 #oriflameturkiye #duoloji   \n",
              "\n",
              "   like_count  comments_count media_type  profile_id    username  post_count  \\\n",
              "0         6.0               0      IMAGE  3170700063  deparmedya         NaN   \n",
              "1        22.0               1      VIDEO  3170700063  deparmedya         NaN   \n",
              "2        19.0               0      VIDEO  3170700063  deparmedya         NaN   \n",
              "3        19.0               1      VIDEO  3170700063  deparmedya         NaN   \n",
              "4        21.0               0      VIDEO  3170700063  deparmedya         NaN   \n",
              "\n",
              "   follower_count  highlight_reel_count  \n",
              "0            1167                     6  \n",
              "1            1167                     6  \n",
              "2            1167                     6  \n",
              "3            1167                     6  \n",
              "4            1167                     6  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load and preprocess the training data\n",
        "training_data_path = 'training-dataset.jsonl'\n",
        "training_data = load_jsonl(training_data_path)\n",
        "\n",
        "# Extract and preprocess data\n",
        "df_combined  = extract_training_data(training_data)\n",
        "df_combined.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec caption vectors shape: (183083, 100)\n",
            "Feature columns: Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
            "       ...\n",
            "       '97', '98', '99', 'comments_count_log', 'media_type_encoded',\n",
            "       'post_count', 'follower_count', 'highlight_reel_count',\n",
            "       'followers_per_post', 'comments_to_followers_ratio'],\n",
            "      dtype='object', length=107)\n",
            "Features standardized.\n"
          ]
        }
      ],
      "source": [
        "X, y, word2vec_model, feature_scaler, feature_columns, media_type_mapping = preprocess_combined_data(df_combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3) XGBoost Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape: (146466, 107), Test set shape: (36617, 107)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape: (146466, 107), Test set shape: (36617, 107)\n",
            "Train Mean Squared Error (MSE): 0.5906\n",
            "Train R-squared (R2): 0.8978\n",
            "Test Mean Squared Error (MSE): 0.7361\n",
            "Test R-squared (R2): 0.8723\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
        "\n",
        "# Instantiate and fit the XGBoost regressor\n",
        "xgb_model = XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=8, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = xgb_model.predict(X_train)\n",
        "y_test_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_test_pred)\n",
        "r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Train Mean Squared Error (MSE): {mean_squared_error(y_train, y_train_pred):.4f}\")\n",
        "print(f\"Train R-squared (R2): {r2_score(y_train, y_train_pred):.4f}\")\n",
        "print(f\"Test Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Test R-squared (R2): {r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #max_depth changed from 5 to 12\n",
        "# from sklearn.metrics import mean_squared_error, r2_score\n",
        "# from xgboost import XGBRegressor\n",
        "\n",
        "# # Instantiate and fit the XGBoost regressor\n",
        "# xgb_model_4 = XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=12, random_state=42)\n",
        "# xgb_model_4.fit(X_train, y_train)\n",
        "\n",
        "# # Make predictions\n",
        "# y_train_pred = xgb_model_4.predict(X_train)\n",
        "# y_test_pred = xgb_model_4.predict(X_test)\n",
        "\n",
        "# # Evaluate the model\n",
        "# mse = mean_squared_error(y_test, y_test_pred)\n",
        "# r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "# print(f\"Train Mean Squared Error (MSE): {mean_squared_error(y_train, y_train_pred):.4f}\")\n",
        "# print(f\"Train R-squared (R2): {r2_score(y_train, y_train_pred):.4f}\")\n",
        "# print(f\"Test Mean Squared Error (MSE): {mse:.4f}\")\n",
        "# print(f\"Test R-squared (R2): {r2:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# from xgboost import XGBRegressor\n",
        "# from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# # Hyperparameter grid\n",
        "# max_depth_values = [8, 10, 12]\n",
        "# n_estimators_values = [100, 200, 300]\n",
        "# learning_rate_values = [0.1, 0.01, 1.5]\n",
        "\n",
        "# # Store results\n",
        "# results = []\n",
        "\n",
        "# # Loop over each combination of hyperparameters\n",
        "# for max_depth in max_depth_values:\n",
        "#     for n_estimators in n_estimators_values:\n",
        "#         for learning_rate in learning_rate_values:\n",
        "#             # Train the model\n",
        "#             model = XGBRegressor(objective='reg:squarederror',\n",
        "#                                  n_estimators=n_estimators,\n",
        "#                                  learning_rate=learning_rate,\n",
        "#                                  max_depth=max_depth,\n",
        "#                                  random_state=42)\n",
        "            \n",
        "#             model.fit(X_train, y_train)\n",
        "\n",
        "#             # Make predictions\n",
        "#             y_train_pred = model.predict(X_train)\n",
        "#             y_test_pred = model.predict(X_test)\n",
        "\n",
        "#             # Evaluate performance\n",
        "#             train_r2 = r2_score(y_train, y_train_pred)\n",
        "#             test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "#             # Append results\n",
        "#             results.append({\n",
        "#                 'max_depth': max_depth,\n",
        "#                 'n_estimators': n_estimators,\n",
        "#                 'learning_rate': learning_rate,\n",
        "#                 'train_r2': train_r2,\n",
        "#                 'test_r2': test_r2\n",
        "#             })\n",
        "\n",
        "# # Convert results to a DataFrame\n",
        "# results_df = pd.DataFrame(results)\n",
        "\n",
        "# # Display the results as a table sorted by test R^2\n",
        "# print(\"Hyperparameter tuning results:\")\n",
        "# print(results_df.sort_values(by='test_r2', ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predictions for test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function for getting profile-level information\n",
        "def get_profile_info(username, df_combined):\n",
        "    return df_combined[df_combined['username'] == username].head(1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_test_data_with_missing(data, word2vec_model, scaler, feature_columns, media_type_mapping, profile_info):\n",
        "    # Create a DataFrame from the single data instance\n",
        "    df_test = pd.DataFrame([data])\n",
        "\n",
        "    # Ensure no None, NaN, or missing values in the 'caption' column\n",
        "    if 'caption' not in df_test.columns:\n",
        "        df_test['caption'] = ''\n",
        "    df_test['caption'] = df_test['caption'].fillna('').astype(str)\n",
        "\n",
        "    # Tokenize and compute Word2Vec embedding\n",
        "    tokenized_caption = df_test['caption'].iloc[0].split()\n",
        "\n",
        "    def get_caption_vector(tokens):\n",
        "        vectors = [word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv]\n",
        "        if vectors:\n",
        "            return np.mean(vectors, axis=0)\n",
        "        else:\n",
        "            return np.zeros(word2vec_model.vector_size)\n",
        "\n",
        "    caption_vector = get_caption_vector(tokenized_caption)\n",
        "    caption_features = pd.DataFrame([caption_vector])\n",
        "\n",
        "    # Add profile-level features\n",
        "    for field in ['post_count', 'follower_count', 'highlight_reel_count', 'followers_per_post', 'comments_to_followers_ratio']:\n",
        "        df_test[field] = profile_info.get(field, 0)\n",
        "\n",
        "    # Handle log transformations\n",
        "    for field in ['post_count', 'follower_count', 'highlight_reel_count', 'comments_count', 'followers_per_post', 'comments_to_followers_ratio']:\n",
        "        df_test[f'{field}_log'] = np.log1p(df_test.get(field, 0))\n",
        "\n",
        "    # Encode media type\n",
        "    if 'media_type' not in df_test.columns:\n",
        "        df_test['media_type'] = \"IMAGE\"\n",
        "    df_test['media_type_encoded'] = df_test['media_type'].map(media_type_mapping).fillna(0)\n",
        "\n",
        "    # Replace infinite values and handle NaNs\n",
        "    df_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    df_test.fillna(0, inplace=True)\n",
        "\n",
        "    # Combine all features\n",
        "    features = caption_features\n",
        "    features.columns = features.columns.astype(str)\n",
        "\n",
        "    # Add engineered features\n",
        "    for feature in ['comments_count_log', 'media_type_encoded', 'post_count_log', 'follower_count_log', \n",
        "                    'highlight_reel_count_log', 'followers_per_post_log', 'comments_to_followers_ratio_log']:\n",
        "        features[feature] = df_test.get(feature, 0)\n",
        "\n",
        "    # Align feature columns with training data order\n",
        "    for col in feature_columns:\n",
        "        if col not in features.columns:\n",
        "            features[col] = 0  # Ensure missing columns are filled with zero\n",
        "\n",
        "    # Ensure column order matches the training feature set\n",
        "    features = features[feature_columns]\n",
        "\n",
        "    # Standardize features\n",
        "    features_scaled = scaler.transform(features)\n",
        "\n",
        "    return features_scaled\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions are completed for test-regression-round3.jsonl\n",
            "Predictions saved to prediction-regression-round4.json\n"
          ]
        }
      ],
      "source": [
        "# Process JSONL file and predict\n",
        "input_file = \"test-regression-round3.jsonl\"\n",
        "output_file = \"prediction-regression-round4.json\"\n",
        "\n",
        "predictions = {}\n",
        "\n",
        "results = {}\n",
        "with open(input_file, 'r') as file:\n",
        "    for line in file:\n",
        "        data = json.loads(line)\n",
        "        profile_info = get_profile_info(data.get(\"username\"), df_combined)\n",
        "        features_scaled = preprocess_test_data_with_missing(data, word2vec_model, feature_scaler, feature_columns, media_type_mapping, profile_info)\n",
        "        # log_like_count = rf_regressor.predict(features_scaled)[0]\n",
        "        log_like_count = xgb_model.predict(features_scaled)[0]\n",
        "        like_count = np.expm1(log_like_count)  # Convert log count back to normal count\n",
        "        results[data['id']] = int(round(like_count))  # Round and convert to integer\n",
        "\n",
        "print(f\"Predictions are completed for {input_file}\")\n",
        "\n",
        "with open(output_file, 'w') as file:\n",
        "    json.dump(results, file, indent=4)\n",
        "# \n",
        "print(f\"Predictions saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 107)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features_scaled.shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
