{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction for Classification\n",
    "\n",
    "- We will first try extract information from the text based data which is collected previously.\n",
    "\n",
    "- Let's load the data and drop the all irrelevant columns (determined in the feature analysis notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import re\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Load the compressed JSON file\n",
    "df = pd.read_json('training-dataset.jsonl.gz', lines=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profile = json_normalize(df['profile'])\n",
    "df_posts = df['posts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_columns = [\n",
    "    # From cell 8\n",
    "    'id', 'bio_links', 'country_block', 'eimu_id', 'external_url', 'fbid',\n",
    "    'profile_pic_url', 'profile_picture_base64', 'fb_profile_biolink',\n",
    "    'show_account_transparency_details', 'business_address_json',\n",
    "    \n",
    "    # From cell 13\n",
    "    'business_phone_number', 'overall_category_name', 'business_email',\n",
    "    'ai_agent_type', 'restricted_by_viewer', 'business_category_name',\n",
    "    'post_count', 'category_enum',\n",
    "    \n",
    "    # From cell 19 (zero_variance_columns)\n",
    "    'is_professional_account', 'is_private', 'is_regulated_c18',\n",
    "    'is_guardian_of_viewer', 'is_supervised_by_viewer', 'is_supervised_user',\n",
    "    'is_embeds_disabled', 'is_joined_recently', 'is_verified_by_mv4b',\n",
    "    'is_supervision_enabled',\n",
    "    \n",
    "    # From cell 26\n",
    "    'business_contact_method',\n",
    "    \n",
    "    # From cell 33\n",
    "    'should_show_category', 'has_clips', 'hide_like_and_view_counts',\n",
    "    'should_show_public_contacts'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profile = df_profile.drop(dropped_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>full_name</th>\n",
       "      <th>biography</th>\n",
       "      <th>category_name</th>\n",
       "      <th>follower_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>is_business_account</th>\n",
       "      <th>is_verified</th>\n",
       "      <th>highlight_reel_count</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [username, full_name, biography, category_name, follower_count, following_count, is_business_account, is_verified, highlight_reel_count, entities]\n",
       "Index: []"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_profile.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_colums = [\"category_name\",\"biography\", \"username\", \"entities\", \"full_name\"]\n",
    "\n",
    "# Fill missing values with empty strings\n",
    "df_profile[text_colums] = df_profile[text_colums].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I will merge all the text based entities into a single block.\n",
    "\n",
    "- Normally category name should be encoded, however, when we encode it we get 432 different categories and that is too much.\n",
    "\n",
    "- We will try to use the same information by concatanating it to bio\n",
    "\n",
    "- Full name also concatanated to bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profile['text_features'] = df_profile['category_name'] + ' ' + df_profile['biography'] + ' ' + df_profile['entities'] + ' ' + df_profile['full_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>full_name</th>\n",
       "      <th>biography</th>\n",
       "      <th>category_name</th>\n",
       "      <th>follower_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>is_business_account</th>\n",
       "      <th>is_verified</th>\n",
       "      <th>highlight_reel_count</th>\n",
       "      <th>entities</th>\n",
       "      <th>text_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [username, full_name, biography, category_name, follower_count, following_count, is_business_account, is_verified, highlight_reel_count, entities, text_features]\n",
       "Index: []"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_profile.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I will proccess the posts, I will concatanate all the posts into a single post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TahaÇAKMAK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       local business #mediaplanning #mediabuying #so...\n",
       "1       personal blog beyaz yakalıların dünyasına hoşg...\n",
       "2       brand sogutozuftz avm ankara macrocenter migro...\n",
       "3       dijital iletişim yönetimiinfo@vimerangcomq dij...\n",
       "4       energy company totalenergies istasyonları resm...\n",
       "                              ...                        \n",
       "5410    healthbeauty şifalı bitkiler atölyesi whatsapp...\n",
       "5411    yildir yuvamiz türkiye fabrikamız çalışanımız ...\n",
       "5412                                     taris zeytinyagı\n",
       "5413    public figure iklimce sohbetlerle ilgili iklim...\n",
       "5414    coffee shop yıl mahişçi blokları opet benzinli...\n",
       "Name: text_features, Length: 5415, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "turkish_stopwords = stopwords.words('turkish')\n",
    "\n",
    "def preprocess_text(text: str):\n",
    "    # Lower casing Turkish Text, Don't use str.lower :)\n",
    "    text = str(text)\n",
    "    text = text.casefold()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    # HERE THE EMOJIS stuff are being removed, you may want to keep them :D\n",
    "    text = re.sub(r'[^a-zçğıöşü0-9\\s#@]', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Remove Turkish stopwords\n",
    "def remove_stopwords(text):\n",
    "    text = str(text)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in turkish_stopwords]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "df_profile['text_features'].apply(preprocess_text).apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = json_normalize(df['posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5415, 35)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store results\n",
    "total_likes = []\n",
    "total_comments = []\n",
    "max_likes = []\n",
    "captions_concatenated = []\n",
    "averages = []\n",
    "\n",
    "\n",
    "for r in range(0, 5415):\n",
    "    total_like = 0\n",
    "    total_comment = 0\n",
    "    max_like = 0\n",
    "    captions = \"\"\n",
    "    for c in range(0, 35):\n",
    "        post = posts.loc[r, c]\n",
    "        # Check if post is not None\n",
    "        if post is not None:\n",
    "            like_count = post.get(\"like_count\", 0)  \n",
    "            comment_count = post.get(\"comments_count\", 0)  \n",
    "\n",
    "            # Calculate total likes and comments, and track the max likes\n",
    "            total_like += int(like_count) if like_count is not None else 0\n",
    "            total_comment += int(comment_count) if comment_count is not None else 0\n",
    "            max_like = max(max_like, int(like_count) if like_count is not None else 0)\n",
    "            captions += post.get(\"caption\", \"\") if post.get(\"caption\") is not None else \"\"\n",
    "            captions += \" \"\n",
    "            \n",
    "\n",
    "    # Append the results for the current user (row)\n",
    "    total_likes.append(total_like)\n",
    "    total_comments.append(total_comment)\n",
    "    max_likes.append(max_like)\n",
    "    averages.append(total_like/35)  \n",
    "    captions_concatenated.append(captions)\n",
    "\n",
    "# After the loop, create a DataFrame to store these results\n",
    "results_df = pd.DataFrame({\n",
    "    'total_likes': total_likes,\n",
    "    'total_comments': total_comments,\n",
    "    'max_likes': max_likes,\n",
    "    'average like': averages,\n",
    "    'captions_concatenated': captions_concatenated\n",
    "})\n",
    "\n",
    "results_df.head()\n",
    "\n",
    "results_df['captions_concatenated'] = results_df['captions_concatenated'].apply(preprocess_text).apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_likes</th>\n",
       "      <th>total_comments</th>\n",
       "      <th>max_likes</th>\n",
       "      <th>average like</th>\n",
       "      <th>captions_concatenated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>404</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>11.542857</td>\n",
       "      <td>cumhuriyetimizin yılı kutlu olsun oriflame duo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1126</td>\n",
       "      <td>33</td>\n",
       "      <td>122</td>\n",
       "      <td>32.171429</td>\n",
       "      <td>diyaloğun yaşanmadığı bir online toplantı olma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1103</td>\n",
       "      <td>13</td>\n",
       "      <td>77</td>\n",
       "      <td>31.514286</td>\n",
       "      <td>bugün bir fincan köpüklü türk kahvesiyle taçla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8186</td>\n",
       "      <td>70</td>\n",
       "      <td>7345</td>\n",
       "      <td>233.885714</td>\n",
       "      <td>saygı özlemle #atatürk #kasım #kasim #vimerang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7960</td>\n",
       "      <td>55905</td>\n",
       "      <td>1250</td>\n",
       "      <td>227.428571</td>\n",
       "      <td>başöğretmenimiz gazi mustafa kemal atatürkün ı...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_likes  total_comments  max_likes  average like  \\\n",
       "0          404              12         26     11.542857   \n",
       "1         1126              33        122     32.171429   \n",
       "2         1103              13         77     31.514286   \n",
       "3         8186              70       7345    233.885714   \n",
       "4         7960           55905       1250    227.428571   \n",
       "\n",
       "                               captions_concatenated  \n",
       "0  cumhuriyetimizin yılı kutlu olsun oriflame duo...  \n",
       "1  diyaloğun yaşanmadığı bir online toplantı olma...  \n",
       "2  bugün bir fincan köpüklü türk kahvesiyle taçla...  \n",
       "3  saygı özlemle #atatürk #kasım #kasim #vimerang...  \n",
       "4  başöğretmenimiz gazi mustafa kemal atatürkün ı...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deparmedya</td>\n",
       "      <td>local business #mediaplanning #mediabuying #so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beyazyakaliyiz</td>\n",
       "      <td>personal blog beyaz yakalıların dünyasına hoşg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kafesfirin</td>\n",
       "      <td>brand sogutozuftz avm ankara macrocenter migro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vimerang</td>\n",
       "      <td>dijital iletişim yönetimiinfo@vimerangcomq dij...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>totalenergies_istasyonlari</td>\n",
       "      <td>energy company totalenergies istasyonları resm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     username  \\\n",
       "0                  deparmedya   \n",
       "1              beyazyakaliyiz   \n",
       "2                  kafesfirin   \n",
       "3                    vimerang   \n",
       "4  totalenergies_istasyonlari   \n",
       "\n",
       "                                       combined_text  \n",
       "0  local business #mediaplanning #mediabuying #so...  \n",
       "1  personal blog beyaz yakalıların dünyasına hoşg...  \n",
       "2  brand sogutozuftz avm ankara macrocenter migro...  \n",
       "3  dijital iletişim yönetimiinfo@vimerangcomq dij...  \n",
       "4  energy company totalenergies istasyonları resm...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe with concatenated text\n",
    "df_combined = pd.DataFrame({\n",
    "    'combined_text': df_profile['text_features'] + ' ' + results_df['captions_concatenated']\n",
    "})\n",
    "\n",
    "# Apply the same preprocessing to keep consistency\n",
    "df_combined['combined_text'] = df_combined['combined_text'].apply(preprocess_text).apply(remove_stopwords)\n",
    "# Concat usernames to the combined text\n",
    "df_combined = pd.concat([df_profile['username'],df_combined], axis=1)\n",
    "\n",
    "\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>taskirancemal</td>\n",
       "      <td>Mom and Children</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tam_kararinda</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spart4nn</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sosyalyiyiciler</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sonaydizdarahad</td>\n",
       "      <td>Mom and Children</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0             label\n",
       "0    taskirancemal  Mom and Children\n",
       "1    tam_kararinda              Food\n",
       "2         spart4nn              Food\n",
       "3  sosyalyiyiciler              Food\n",
       "4  sonaydizdarahad  Mom and Children"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training classification data\n",
    "training_data = pd.read_csv('train-classification.csv')\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deparmedya</td>\n",
       "      <td>local business #mediaplanning #mediabuying #so...</td>\n",
       "      <td>Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kafesfirin</td>\n",
       "      <td>brand sogutozuftz avm ankara macrocenter migro...</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vimerang</td>\n",
       "      <td>dijital iletişim yönetimiinfo@vimerangcomq dij...</td>\n",
       "      <td>Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mustafa_yalcinn38</td>\n",
       "      <td>politician talas belediye baskanı talas beledi...</td>\n",
       "      <td>Health and Lifestyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zorluenergysolutions</td>\n",
       "      <td>türkiyenin ilindeki yaygın elektrikli şarj ist...</td>\n",
       "      <td>Tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               username                                      combined_text  \\\n",
       "0            deparmedya  local business #mediaplanning #mediabuying #so...   \n",
       "1            kafesfirin  brand sogutozuftz avm ankara macrocenter migro...   \n",
       "2              vimerang  dijital iletişim yönetimiinfo@vimerangcomq dij...   \n",
       "3     mustafa_yalcinn38  politician talas belediye baskanı talas beledi...   \n",
       "4  zorluenergysolutions  türkiyenin ilindeki yaygın elektrikli şarj ist...   \n",
       "\n",
       "                  label  \n",
       "0                  Tech  \n",
       "1                  Food  \n",
       "2                  Tech  \n",
       "3  Health and Lifestyle  \n",
       "4                  Tech  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the column Unnamed: 0 to username\n",
    "training_data = training_data.rename(columns={'Unnamed: 0': 'username'})\n",
    "training_data.head()\n",
    "# Merge the df_profile and training_data dataframes\n",
    "df_combined = pd.merge(df_combined, training_data, on='username')\n",
    "\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all labels to lowercase for consistency\n",
    "df_combined['label'] = df_combined['label'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for TF-IDF + Random Forest:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                 art       0.31      0.13      0.19        38\n",
      "       entertainment       0.36      0.37      0.37        59\n",
      "             fashion       0.57      0.75      0.65        55\n",
      "                food       0.76      0.93      0.83       114\n",
      "              gaming       0.00      0.00      0.00         5\n",
      "health and lifestyle       0.51      0.67      0.58        96\n",
      "    mom and children       1.00      0.03      0.06        34\n",
      "              sports       1.00      0.15      0.26        27\n",
      "                tech       0.62      0.80      0.70        59\n",
      "              travel       0.77      0.66      0.71        62\n",
      "\n",
      "            accuracy                           0.60       549\n",
      "           macro avg       0.59      0.45      0.43       549\n",
      "        weighted avg       0.63      0.60      0.56       549\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Public\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Public\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Public\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load your dataframe\n",
    "# df = pd.read_csv('your_file.csv')  # Uncomment and load your file\n",
    "\n",
    "# Preprocessing\n",
    "X = df_combined['combined_text']\n",
    "y = df_combined['label']\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorizing the text\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Model training\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Classification Report for TF-IDF + Random Forest:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Word2Vec + Random Forest:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                 art       0.33      0.13      0.19        38\n",
      "       entertainment       0.30      0.42      0.35        59\n",
      "             fashion       0.45      0.64      0.53        55\n",
      "                food       0.77      0.71      0.74       114\n",
      "              gaming       0.00      0.00      0.00         5\n",
      "health and lifestyle       0.48      0.61      0.54        96\n",
      "    mom and children       0.60      0.09      0.15        34\n",
      "              sports       0.45      0.19      0.26        27\n",
      "                tech       0.49      0.68      0.57        59\n",
      "              travel       0.56      0.45      0.50        62\n",
      "\n",
      "            accuracy                           0.51       549\n",
      "           macro avg       0.45      0.39      0.38       549\n",
      "        weighted avg       0.52      0.51      0.49       549\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Public\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Public\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Public\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize the text data\n",
    "def tokenize_text(text):\n",
    "    return text.split()\n",
    "\n",
    "# Prepare sentences for Word2Vec\n",
    "sentences = df_combined['combined_text'].apply(tokenize_text).values\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences, \n",
    "                    vector_size=100,  # Dimension of word vectors\n",
    "                    window=5,         # Context window size\n",
    "                    min_count=1,      # Ignore words that appear less than this\n",
    "                    workers=4)        # Number of threads to train the model\n",
    "\n",
    "# Function to create document vectors by averaging word vectors\n",
    "def get_doc_vector(text, model):\n",
    "    words = text.split()\n",
    "    word_vecs = []\n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            word_vecs.append(model.wv[word])\n",
    "    if len(word_vecs) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(word_vecs, axis=0)\n",
    "\n",
    "# Create document vectors for all texts\n",
    "X_w2v = np.array([get_doc_vector(text, w2v_model) for text in df_combined['combined_text']])\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_w2v_scaled = scaler.fit_transform(X_w2v)\n",
    "\n",
    "# Split the data\n",
    "X_train_w2v, X_test_w2v, y_train, y_test = train_test_split(X_w2v_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest with Word2Vec features\n",
    "rf_w2v = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_w2v.fit(X_train_w2v, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_w2v = rf_w2v.predict(X_test_w2v)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report for Word2Vec + Random Forest:\")\n",
    "print(classification_report(y_test, y_pred_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 137/137 [01:34<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.4909805912170968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 137/137 [01:36<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.8066387352717184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 137/137 [01:38<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.49136408722966257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 137/137 [01:37<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.2871207228324709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 137/137 [01:37<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.1480402083741161\n",
      "\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                 art       0.40      0.74      0.52        38\n",
      "       entertainment       0.39      0.41      0.40        59\n",
      "             fashion       0.74      0.84      0.79        55\n",
      "                food       0.82      0.93      0.87       114\n",
      "              gaming       1.00      0.60      0.75         5\n",
      "health and lifestyle       0.90      0.45      0.60        96\n",
      "    mom and children       0.50      0.38      0.43        34\n",
      "              sports       0.88      0.78      0.82        27\n",
      "                tech       0.75      0.83      0.79        59\n",
      "              travel       0.72      0.69      0.70        62\n",
      "\n",
      "            accuracy                           0.68       549\n",
      "           macro avg       0.71      0.66      0.67       549\n",
      "        weighted avg       0.72      0.68      0.68       549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('dbmdz/bert-base-turkish-uncased')\n",
    "bert = AutoModel.from_pretrained('dbmdz/bert-base-turkish-uncased')\n",
    "\n",
    "# Custom Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label)\n",
    "        }\n",
    "\n",
    "# BERT Classifier\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(bert.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[0][:, 0]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Prepare data\n",
    "# Convert labels to numerical format\n",
    "label_dict = {label: idx for idx, label in enumerate(df_combined['label'].unique())}\n",
    "y_numeric = df_combined['label'].map(label_dict)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_combined['combined_text'], \n",
    "    y_numeric, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer)\n",
    "test_dataset = TextDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BERTClassifier(bert, len(label_dict))\n",
    "model = model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}'):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "actual = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        actual.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert numeric predictions back to labels\n",
    "reverse_label_dict = {v: k for k, v in label_dict.items()}\n",
    "predictions = [reverse_label_dict[p] for p in predictions]\n",
    "actual = [reverse_label_dict[a] for a in actual]\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(actual, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 137/137 [01:35<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Average training loss: 0.0570\n",
      "Average validation loss: 1.5067\n",
      "\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                 art       0.49      0.66      0.56        38\n",
      "       entertainment       0.48      0.27      0.35        59\n",
      "             fashion       0.65      0.87      0.74        55\n",
      "                food       0.86      0.89      0.88       114\n",
      "              gaming       1.00      0.60      0.75         5\n",
      "health and lifestyle       0.69      0.67      0.68        96\n",
      "    mom and children       0.56      0.41      0.47        34\n",
      "              sports       0.87      0.74      0.80        27\n",
      "                tech       0.71      0.81      0.76        59\n",
      "              travel       0.80      0.79      0.80        62\n",
      "\n",
      "            accuracy                           0.71       549\n",
      "           macro avg       0.71      0.67      0.68       549\n",
      "        weighted avg       0.70      0.71      0.70       549\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 137/137 [01:37<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Average training loss: 0.0284\n",
      "Average validation loss: 1.7672\n",
      "\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                 art       0.49      0.55      0.52        38\n",
      "       entertainment       0.48      0.25      0.33        59\n",
      "             fashion       0.65      0.87      0.74        55\n",
      "                food       0.85      0.90      0.88       114\n",
      "              gaming       1.00      0.80      0.89         5\n",
      "health and lifestyle       0.77      0.65      0.70        96\n",
      "    mom and children       0.49      0.50      0.49        34\n",
      "              sports       0.84      0.78      0.81        27\n",
      "                tech       0.70      0.83      0.76        59\n",
      "              travel       0.72      0.76      0.74        62\n",
      "\n",
      "            accuracy                           0.70       549\n",
      "           macro avg       0.70      0.69      0.69       549\n",
      "        weighted avg       0.70      0.70      0.69       549\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 137/137 [01:37<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Average training loss: 0.0085\n",
      "Average validation loss: 1.7789\n",
      "\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                 art       0.55      0.45      0.49        38\n",
      "       entertainment       0.45      0.47      0.46        59\n",
      "             fashion       0.66      0.84      0.74        55\n",
      "                food       0.85      0.90      0.88       114\n",
      "              gaming       1.00      0.60      0.75         5\n",
      "health and lifestyle       0.80      0.58      0.67        96\n",
      "    mom and children       0.56      0.44      0.49        34\n",
      "              sports       0.85      0.81      0.83        27\n",
      "                tech       0.72      0.81      0.76        59\n",
      "              travel       0.67      0.77      0.72        62\n",
      "\n",
      "            accuracy                           0.70       549\n",
      "           macro avg       0.71      0.67      0.68       549\n",
      "        weighted avg       0.71      0.70      0.70       549\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 137/137 [01:37<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Average training loss: 0.0016\n",
      "Average validation loss: 1.8745\n",
      "\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                 art       0.52      0.61      0.56        38\n",
      "       entertainment       0.49      0.34      0.40        59\n",
      "             fashion       0.68      0.87      0.76        55\n",
      "                food       0.87      0.90      0.89       114\n",
      "              gaming       1.00      0.80      0.89         5\n",
      "health and lifestyle       0.78      0.67      0.72        96\n",
      "    mom and children       0.57      0.59      0.58        34\n",
      "              sports       0.81      0.78      0.79        27\n",
      "                tech       0.73      0.83      0.78        59\n",
      "              travel       0.77      0.76      0.76        62\n",
      "\n",
      "            accuracy                           0.73       549\n",
      "           macro avg       0.72      0.71      0.71       549\n",
      "        weighted avg       0.72      0.73      0.72       549\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 137/137 [01:37<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "Average training loss: 0.0006\n",
      "Average validation loss: 1.9171\n",
      "\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                 art       0.51      0.63      0.56        38\n",
      "       entertainment       0.49      0.36      0.41        59\n",
      "             fashion       0.70      0.87      0.77        55\n",
      "                food       0.87      0.89      0.88       114\n",
      "              gaming       1.00      0.80      0.89         5\n",
      "health and lifestyle       0.73      0.66      0.69        96\n",
      "    mom and children       0.58      0.44      0.50        34\n",
      "              sports       0.84      0.78      0.81        27\n",
      "                tech       0.68      0.85      0.76        59\n",
      "              travel       0.80      0.76      0.78        62\n",
      "\n",
      "            accuracy                           0.72       549\n",
      "           macro avg       0.72      0.70      0.71       549\n",
      "        weighted avg       0.72      0.72      0.71       549\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 137/137 [01:37<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "Average training loss: 0.0005\n",
      "Average validation loss: 2.0398\n",
      "\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                 art       0.46      0.66      0.54        38\n",
      "       entertainment       0.47      0.31      0.37        59\n",
      "             fashion       0.70      0.85      0.77        55\n",
      "                food       0.87      0.89      0.88       114\n",
      "              gaming       1.00      0.80      0.89         5\n",
      "health and lifestyle       0.71      0.70      0.71        96\n",
      "    mom and children       0.50      0.38      0.43        34\n",
      "              sports       0.88      0.78      0.82        27\n",
      "                tech       0.73      0.81      0.77        59\n",
      "              travel       0.80      0.76      0.78        62\n",
      "\n",
      "            accuracy                           0.71       549\n",
      "           macro avg       0.71      0.69      0.70       549\n",
      "        weighted avg       0.71      0.71      0.71       549\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 137/137 [01:37<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "Average training loss: 0.0011\n",
      "Average validation loss: 1.9643\n",
      "\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                 art       0.55      0.58      0.56        38\n",
      "       entertainment       0.47      0.41      0.44        59\n",
      "             fashion       0.69      0.87      0.77        55\n",
      "                food       0.87      0.89      0.88       114\n",
      "              gaming       1.00      0.80      0.89         5\n",
      "health and lifestyle       0.76      0.66      0.70        96\n",
      "    mom and children       0.62      0.47      0.53        34\n",
      "              sports       0.84      0.78      0.81        27\n",
      "                tech       0.71      0.86      0.78        59\n",
      "              travel       0.77      0.76      0.76        62\n",
      "\n",
      "            accuracy                           0.72       549\n",
      "           macro avg       0.73      0.71      0.71       549\n",
      "        weighted avg       0.72      0.72      0.72       549\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 137/137 [01:37<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "Average training loss: 0.0003\n",
      "Average validation loss: 1.9943\n",
      "\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                 art       0.55      0.63      0.59        38\n",
      "       entertainment       0.50      0.41      0.45        59\n",
      "             fashion       0.70      0.87      0.77        55\n",
      "                food       0.87      0.89      0.88       114\n",
      "              gaming       1.00      0.80      0.89         5\n",
      "health and lifestyle       0.75      0.67      0.71        96\n",
      "    mom and children       0.58      0.41      0.48        34\n",
      "              sports       0.84      0.78      0.81        27\n",
      "                tech       0.70      0.86      0.77        59\n",
      "              travel       0.78      0.76      0.77        62\n",
      "\n",
      "            accuracy                           0.73       549\n",
      "           macro avg       0.73      0.71      0.71       549\n",
      "        weighted avg       0.72      0.73      0.72       549\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 137/137 [01:37<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "Average training loss: 0.0003\n",
      "Average validation loss: 2.0121\n",
      "\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                 art       0.51      0.63      0.56        38\n",
      "       entertainment       0.51      0.39      0.44        59\n",
      "             fashion       0.70      0.87      0.77        55\n",
      "                food       0.87      0.89      0.88       114\n",
      "              gaming       1.00      0.80      0.89         5\n",
      "health and lifestyle       0.75      0.66      0.70        96\n",
      "    mom and children       0.62      0.47      0.53        34\n",
      "              sports       0.84      0.78      0.81        27\n",
      "                tech       0.70      0.86      0.77        59\n",
      "              travel       0.80      0.76      0.78        62\n",
      "\n",
      "            accuracy                           0.73       549\n",
      "           macro avg       0.73      0.71      0.71       549\n",
      "        weighted avg       0.73      0.73      0.72       549\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 137/137 [01:37<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "Average training loss: 0.0003\n",
      "Average validation loss: 2.0156\n",
      "\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                 art       0.50      0.63      0.56        38\n",
      "       entertainment       0.50      0.37      0.43        59\n",
      "             fashion       0.70      0.87      0.77        55\n",
      "                food       0.87      0.89      0.88       114\n",
      "              gaming       1.00      0.80      0.89         5\n",
      "health and lifestyle       0.75      0.66      0.70        96\n",
      "    mom and children       0.62      0.47      0.53        34\n",
      "              sports       0.84      0.78      0.81        27\n",
      "                tech       0.70      0.86      0.77        59\n",
      "              travel       0.80      0.76      0.78        62\n",
      "\n",
      "            accuracy                           0.72       549\n",
      "           macro avg       0.73      0.71      0.71       549\n",
      "        weighted avg       0.72      0.72      0.72       549\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "WARMUP_STEPS = 0.1  # 10% of total steps\n",
    "WEIGHT_DECAY = 0.01\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "# Update model's dropout\n",
    "model.bert.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "model.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "\n",
    "# Calculate total steps for scheduler\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_STEPS)\n",
    "\n",
    "# Initialize optimizer with weight decay\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': WEIGHT_DECAY\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# Training loop with validation\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{EPOCHS}'):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    predictions = []\n",
    "    actual = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            actual.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(test_loader)\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}:')\n",
    "    print(f'Average training loss: {avg_train_loss:.4f}')\n",
    "    print(f'Average validation loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Convert predictions back to labels and print classification report\n",
    "    predictions = [reverse_label_dict[p] for p in predictions]\n",
    "    actual = [reverse_label_dict[a] for a in actual]\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(actual, predictions))\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
